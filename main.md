# 2020.7.20 浙江大华算法工程师一面（CV方向）

今天秋招面试开张，面试体验良好，只是一开始问的几个数学问题实在没想到，好在自我感觉回答得还行。

做完自我介绍后面试官像是早有预谋般地问“条件随机场了解吗”（大概说了一下属于判别方法，极大团，然后就说不出来了。我每次都抱着侥幸的心理不想看CRF，甚至莫名地排斥这个方法，今天墨菲定律终于显灵了。）

之后问的问题完全没有条理性，纯属想到哪儿问到哪儿。大概问题如下：

1. > Hessian 矩阵了解吗，介绍一下 Heissian 矩阵与鞍点的关系
  
  函数对自变量求二阶导；特征值有正有负
  
2. > 矩阵可逆的条件

  （线代为数不多还记得的知识点）列向量线性无关、矩阵满秩、特征值没有 0 ，行列式不为 0，这几个互相等价
  
3. > 介绍一下交叉熵，还有哪些衡量分布差异的量

  KL 散度
  
  > 介绍一下 KL 散度
  
  记不清了，只记得大概形式与交叉熵有关，面试完翻了翻书： $H(p,q) = D_{KL}(P||Q)+H(P)$
  
4. > 介绍一下权重衰减

（简单说了一下，但没让解释）先验分布和解空间形状两个方面

5. > 了解哪些网络结构

  前馈，CNN，RNN，残差网络，LSTM，GRU，Seq2seq
  
6. > 了解轻量级网络结构吗

  没听过……
  
7. > 说一下 Sigmoid 为什么用的不多，有什么缺点

  和 tanh 一样，会饱和，导致梯度消失
  
8. > C 和 C++ 会吗？

  不会

9. > 平时用什么语言

  python 和 R
  
  > 用过哪些神经网络框架？
  
  pytorch

10. > 出了一道巨简单的考察 Python list 复制操作的题

11. > 深拷贝和浅拷贝

  浅拷贝只做最顶层的数据类型判断，深拷贝做递归拷贝；遇到可变类型则创建新的内存空间，遇到不可变数据类型就是拷贝的引用。（之前平安科技也面过这个问题，没想到这么常考）
  
12. > Linux 会吗？列出文件目录是什么命令？按时间顺序列出文件？统计目录下文件个数？

  （后俩都不会，linux 基础太差了）
  
13. > 说一下残差网络，在哪儿学的？
    
   博客和书，然后说了一下有些CNN结构也常用，transformer也用到了
    
14. > pytorch 里哪些结构在训练和测试阶段有区别？

  dropout 和 bn，自我感觉解释得挺好的，把 train 和 eval 也说了
  
15. > CNN 怎么做 BN？

  就按 batch size 压缩呗

16. > BN 最后有个线性变换，为什么加线性变换？

  （如果后面接 Sigmoid，会损失非线性效果。由数据学习变换的参数，可以提高网络表达能力）
  
17. > 你觉得你有什么理由说服我你能胜任这个岗位？

对自己的学习能力一顿乱夸

18. > 介绍一下实习项目

  balabala 侃了一大堆，结果也没问相关的问题…
  
19. > 学过哪些和机器学习相关的课程？

  统计学习、算法导论、数据结构……
  
20. > 给定数据 X 和 label Y，用线性回归怎么求解权重参数？

  正规方程 $W = (X^\top X)^{-1}X^\top Y$，亏我还记得
  
21. > 还用过哪些基于梯度的优化算法？

Adagrad，RMSProp，Adam，Adam 用的比较多，效果最好

  > Adam 为什么效果好，有哪些改进？
  
  Adam 基于 RMSPorp，每个参数有一个自适应的learning rate，同时累计了一阶和二阶梯度的信息，还进行了无偏的修正。
  
  > 基于动量的方法有什么好处？
  
  基于动量的方法可以累积历史的梯度，加速训练，同时还能跃过一些局部极小点。
  
22. > 讲一下 boosting

  boosting 是一个集成学习方法，由基学习器累加而成。通过每一步学习一个基学习器，最终得到一个精度较高的学习器。
  
  > 和随机森林有什么不同？
  
  随机森林是基于 bagging 方法的模型，可以并行训练，而 boosting 只能串行训练。随机森林的每个学习器都是一棵低偏差高方差的分类或回归树，通过取平均来得到最终的结果。这样取平均可以降低模型的方差，避免过拟合。
  
23. > 有什么问题要问我？

   问了组里做的工作（目标检测，人脸识别，活体检测）；预研阶段用 python ，落地用 C 或者 C++；虽然用的大多都是 deep 的方法，但也希望我多了解传统计算机视觉的方法和思想。
   
##  总结

面试官问的问题难度尚可，面试时长一个小时，没问什么奇奇怪怪的问题。除了上述问题以外，还问过我看没看过某篇论文，虽然最近抽空看了不少论文和源码，但总做不到面面俱到。不会的问题我都老老实实交代，免得被追问起来更尴尬。不过今天面试过程中我和面试官之间几乎没有双向的交流，他问我答，略微有点儿压力。好在秋招第一面下来，自己发现了不少问题，可以早点儿查漏补缺以免后面挂在心仪的公司上。


# 2020.7.29 网易互娱机器学习工程师一面

面试官人很nice，问的问题很基础，大概半个小时结束。

1. > 自我介绍、介绍项目

2. > 缺失值怎么处理？

  可以采用树模型这类对缺失值不敏感的模型；采用均值或中位数填充；也可以采用规则，分层进行填充；（当时忘了说把缺失值单独作为一类，新提一个特征了）
  
3. > 介绍stacking集成学习

4. > 反转链表（不额外占用空间）

  用递归实现了（后来想了一下，其实可以额外开一个dummy 节点，重新创建一个链表，倒也不算额外占用空间）
  
5. > 一道 SQL 题（不会）

6. > 手写逻辑回归的损失函数和导数

7. > 一道开放题，给定一个黑盒神经网络（结构未知），输入是一个double数组，问如何找到神经网络的最小输出值

  想到了 SMO 算法和坐标轴下降法，大概说了一下固定其他输入的维数，只单独对某一个维度的输入进行线性搜索，找到其极小值点，然后迭代直至收敛。
  
## 总结

总体还是比较简单轻松的，甚至比找实习的一堆面试还要简单。忍不住再夸一遍面试官，人真的不错，很和蔼，面试起来也没什么压力，甚至能够享受面试中和面试官交流的过程。

# 2020.7.29 作业帮机器学习算法工程师一面

面试长达一小时，也还比较轻松

1. > 自我介绍，介绍项目

  面试官对于业务挖得有点儿深（好奇是不是打探机密的），问 lightgbm 和随机森林的区别，lightgbm 如何分裂，lightgbm 的优化之处在哪儿
  
2. > Transformer

  我给自己挖了个坑，还好又跳出来了。面试官问有没有了解深度学习方面的东西，我回答说前馈神经网络、CNN、RNN，甚至深度的NLP模型都有了解。然后面试官开始挖 Transformer，问Transformer 解决了什么问题，Transformer 的 self-attention 怎么做？ Decoder 的输入是什么？ Decoder 为什么要 Masked，如何实现的？ Transformer 的损失函数是什么？（softmax多分类的对数损失函数）
  
3. > 算法题

  第一题又是反转链表，反转链表这么火吗？第二题Leetcode原题，[分割回文串](https://leetcode-cn.com/problems/palindrome-partitioning/)，但我居然没写出来！和面试官交流了一下，其实思路是对的，但细节没处理好。
  
## 总结

作业帮的这位面试官人也很好，非常耐心，帮我一起改bug……似乎今年算法岗的“灰飞烟灭”没有网上说的那么严重，至少今天的两轮面试不算什么地狱难度。


# 2020.7.30 新浪算法工程师一面

面了一个多小时，面试官提的问题很有水平，也很理解应届生的水平的情况。面试官非常健谈，面试的过程几乎得有30%的时间是他在说，而我回答问题就像在抛砖引玉。面试官人好到我想和他做朋友！

1. > 自我介绍，介绍项目

  （项目还没介绍完就开始问我问题了）
  
2. > 让我介绍随机森林、GBDT、Lightgbm，想到啥说啥

  老问题了，叽里呱啦说了一大堆，从 Bagging, Boosting 角度说，从 Lightgbm、 XGboost的加速训练、正则化优化角度说。
  
3. > 面试官让我自己挑几个我比较熟的方向

  我说我对传统机器学习方法、深度的方法都还算熟，对NLP也有一定了解
  
4. > 开始问 CNN，问了一个卷积核、stride、padding和feature map大小关系的问题

  画了个图，简单推了一下，没啥问题
  
5. > 问空洞卷积，有什么作用？
  
  我回答出了扩大感受野的范围，但其实我不太记得空洞卷积和转置卷积的差别了，贴两篇文章在这儿
  
  [空洞卷积](https://zhuanlan.zhihu.com/p/50369448)
  
  [转置卷积](https://zhuanlan.zhihu.com/p/48501100)
  
5. > 问 RNN 怎么做 BP
  
  这个问题问得绝了，我只记得叫 BPTT，但没细看过……面试官回答说没关系，这个问题有点儿难
  
6. > 问文本怎么套 CNN ？
  
  像滑窗一样（类似 n-gram）
  
  > 长度是文本序列，那宽度是什么？
  
  不记得了，但灵机一动，说是 Embedding 后的维度（后来查了一下，好像TextCNN真是这么做的）
  
7. > 让我聊对 Transformer 的理解
  
  我从 RNN，LSTM 开始聊，聊到长距离依赖问题，然后说到 attention 和 self-attention 的区别，transformer 可以提特征 Balabala
  
  此时面试官和我聊为什么很多 Pre-trained 模型在 fine-tuning 过程中效果不好。我说可能预训练的数据和实际使用的数据差别比较大，比如 GPT2 在数据上花了很大功夫。面试官说了他的看法，有时候深度纯粹是玄学，可能 Encoder 或者 Decoder 少一层多一层效果就差很大，他建议应届生可以多实践一下。不过他也鼓励我说，没关系，应届生很多都不懂很正常，只要基础好，随便学学都能学会。当时非常感动，居然有这么善良的面试官！
  
8. > 考了几道编程题
  
  牛顿法求平方根（完全忘了牛顿法是什么了，和他交流了一下，还是不会……于是他转而让我写二分）
  （复盘了一下， $x_{n+1} = x_{n}-\frac{f(x_n)}{f'(x_n)}$）
  
  有序数组手写二分（这个还算简单），在有翻转的数组下写二分法（第二个花了点儿时间，但还是写出来了）
  
我反问他：

1. > 组里在做什么

  他在新浪新闻客户端，主要做文本分类，命名实体识别。对每条新闻，判断是否涉黄涉政，是否属于某个新闻板块下某个小板块。他的经验是，很多深度的东西总归还是不如规则来的简单方便。
 
2. > 问他为什么从百度跳到新浪
  
  他回答说跳出来歇歇脚，工作强度低一些，不过建议应届生还是要去大厂多学点儿东西。他举例说他们部门不比新浪微博部门，资源比较少，不是个很能锻炼人的地方，甚至不太建议我来新浪（听到这儿我简直要给他跪下，头一次听见面试官劝退求职者的，巨感动，巨良心！）
  
## 总结

面试以来的面试官一个比一个好，不过这是我面试以来遇见的最棒的面试官，没有之一！特地通过微信和他认真地道谢，面试的过程像是一位长者指引后辈前路一般，能够经历一次这种面试也是人生的幸运。
  
  
# 2020.7.31 大华算法工程师一面（音频方向）

相比于昨天新浪的神级面试官，今天的大华面试官就平凡很多。让我比较吃惊的是，全程几乎没问技术性问题……

先是自我介绍，之后问我用什么语言（R 和 python），问会不会 C 和 C++（学过 C ，忘了），套路和一面如出一辙。之后问实习项目，我都快把我的实习情况背下来了…但面试官老是喜欢打断我，让我有点儿不太爽。这位面试官是头一个问我论文的，我就简单和他说了一下什么是高斯过程回归，怎么定义协方差矩阵，怎么训练，怎么预测（有点儿开组会讲论文的感觉了）。

之后还问了几个很老套的问题，比如有什么兴趣爱好，压力比较大的时候会做什么，如果让你一个人负责一个比较紧急的项目你会怎么做（先多方面学习，多看论文博客源代码，然后硬着头皮做呗），还问我觉得我人生中比较成功的地方有哪些，比较失败的地方有哪些（头一次在技术面遇到这种问题，我以为得等到 HR 面才会被问）。最后问了我的职业规划（还是老套路）。

然后反问，我先问组里做什么（语音识别），是什么让面试官留在大华（开始画大饼，说可以接触到不同的项目，对于项目理解更深，成长更快）。最后明显觉得面试官有点儿想留我，说你统计机器学习建模挺熟练的，做这个应该没什么问题（这根本就不是一回事儿嘛）
