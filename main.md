# 2020.7.20 浙江大华算法工程师一面（CV方向）

今天秋招面试开张，面试体验良好，只是一开始问的几个数学问题实在没想到，好在自我感觉回答得还行。

做完自我介绍后面试官像是早有预谋般地问“条件随机场了解吗”（大概说了一下属于判别方法，极大团，然后就说不出来了。我每次都抱着侥幸的心理不想看CRF，甚至莫名地排斥这个方法，今天墨菲定律终于显灵了。）

之后问的问题完全没有条理性，纯属想到哪儿问到哪儿。大概问题如下：

1. > Hessian 矩阵了解吗，介绍一下 Heissian 矩阵与鞍点的关系
  
  函数对自变量求二阶导；特征值有正有负
  
2. > 矩阵可逆的条件

  （线代为数不多还记得的知识点）列向量线性无关、矩阵满秩、特征值没有 0 ，行列式不为 0，这几个互相等价
  
3. > 介绍一下交叉熵，还有哪些衡量分布差异的量

  KL 散度
  
  > 介绍一下 KL 散度
  
  记不清了，只记得大概形式与交叉熵有关，面试完翻了翻书： $H(p,q) = D_{KL}(P||Q)+H(P)$
  
4. > 介绍一下权重衰减

（简单说了一下，但没让解释）先验分布和解空间形状两个方面

5. > 了解哪些网络结构

  前馈，CNN，RNN，残差网络，LSTM，GRU，Seq2seq
  
6. > 了解轻量级网络结构吗

  没听过……
  
7. > 说一下 Sigmoid 为什么用的不多，有什么缺点

  和 tanh 一样，会饱和，导致梯度消失
  
8. > C 和 C++ 会吗？

  不会

9. > 平时用什么语言

  python 和 R
  
  > 用过哪些神经网络框架？
  
  pytorch

10. > 出了一道巨简单的考察 Python list 复制操作的题

11. > 深拷贝和浅拷贝

  浅拷贝只做最顶层的数据类型判断，深拷贝做递归拷贝；遇到可变类型则创建新的内存空间，遇到不可变数据类型就是拷贝的引用。（之前平安科技也面过这个问题，没想到这么常考）
  
12. > Linux 会吗？列出文件目录是什么命令？按时间顺序列出文件？统计目录下文件个数？

  （后俩都不会，linux 基础太差了）
  
13. > 说一下残差网络，在哪儿学的？
  
   博客和书，然后说了一下有些CNN结构也常用，transformer也用到了
   
14. > pytorch 里哪些结构在训练和测试阶段有区别？

  dropout 和 bn，自我感觉解释得挺好的，把 train 和 eval 也说了
  
15. > CNN 怎么做 BN？

  就按 batch size 压缩呗

16. > BN 最后有个线性变换，为什么加线性变换？

  （如果后面接 Sigmoid，会损失非线性效果。由数据学习变换的参数，可以提高网络表达能力）
  
17. > 你觉得你有什么理由说服我你能胜任这个岗位？

对自己的学习能力一顿乱夸

18. > 介绍一下实习项目

  balabala 侃了一大堆，结果也没问相关的问题…
  
19. > 学过哪些和机器学习相关的课程？

  统计学习、算法导论、数据结构……
  
20. > 给定数据 X 和 label Y，用线性回归怎么求解权重参数？

  正规方程 $W = (X^\top X)^{-1}X^\top Y$，亏我还记得
  
21. > 还用过哪些基于梯度的优化算法？

Adagrad，RMSProp，Adam，Adam 用的比较多，效果最好

  > Adam 为什么效果好，有哪些改进？

  Adam 基于 RMSPorp，每个参数有一个自适应的learning rate，同时累计了一阶和二阶梯度的信息，还进行了无偏的修正。

  > 基于动量的方法有什么好处？

  基于动量的方法可以累积历史的梯度，加速训练，同时还能跃过一些局部极小点。

22. > 讲一下 boosting

  boosting 是一个集成学习方法，由基学习器累加而成。通过每一步学习一个基学习器，最终得到一个精度较高的学习器。
  
  > 和随机森林有什么不同？
  
  随机森林是基于 bagging 方法的模型，可以并行训练，而 boosting 只能串行训练。随机森林的每个学习器都是一棵低偏差高方差的分类或回归树，通过取平均来得到最终的结果。这样取平均可以降低模型的方差，避免过拟合。
  
23. > 有什么问题要问我？

   问了组里做的工作（目标检测，人脸识别，活体检测）；预研阶段用 python ，落地用 C 或者 C++；虽然用的大多都是 deep 的方法，但也希望我多了解传统计算机视觉的方法和思想。
   
##  总结

面试官问的问题难度尚可，面试时长一个小时，没问什么奇奇怪怪的问题。除了上述问题以外，还问过我看没看过某篇论文，虽然最近抽空看了不少论文和源码，但总做不到面面俱到。不会的问题我都老老实实交代，免得被追问起来更尴尬。不过今天面试过程中我和面试官之间几乎没有双向的交流，他问我答，略微有点儿压力。好在秋招第一面下来，自己发现了不少问题，可以早点儿查漏补缺以免后面挂在心仪的公司上。


# 2020.7.29 网易互娱机器学习工程师一面

面试官人很nice，问的问题很基础，大概半个小时结束。

1. > 自我介绍、介绍项目

2. > 缺失值怎么处理？

  可以采用树模型这类对缺失值不敏感的模型；采用均值或中位数填充；也可以采用规则，分层进行填充；（当时忘了说把缺失值单独作为一类，新提一个特征了）
  
3. > 介绍stacking集成学习

4. > 反转链表（不额外占用空间）

  用递归实现了（后来想了一下，其实可以额外开一个dummy 节点，重新创建一个链表，倒也不算额外占用空间）
  
5. > 一道 SQL 题（不会）

6. > 手写逻辑回归的损失函数和导数

7. > 一道开放题，给定一个黑盒神经网络（结构未知），输入是一个double数组，问如何找到神经网络的最小输出值

  想到了 SMO 算法和坐标轴下降法，大概说了一下固定其他输入的维数，只单独对某一个维度的输入进行线性搜索，找到其极小值点，然后迭代直至收敛。
  
## 总结

总体还是比较简单轻松的，甚至比找实习的一堆面试还要简单。忍不住再夸一遍面试官，人真的不错，很和蔼，面试起来也没什么压力，甚至能够享受面试中和面试官交流的过程。

# 2020.7.29 作业帮机器学习算法工程师一面

面试长达一小时，也还比较轻松

1. > 自我介绍，介绍项目

  面试官对于业务挖得有点儿深（好奇是不是打探机密的），问 lightgbm 和随机森林的区别，lightgbm 如何分裂，lightgbm 的优化之处在哪儿
  
2. > Transformer

  我给自己挖了个坑，还好又跳出来了。面试官问有没有了解深度学习方面的东西，我回答说前馈神经网络、CNN、RNN，甚至深度的NLP模型都有了解。然后面试官开始挖 Transformer，问Transformer 解决了什么问题，Transformer 的 self-attention 怎么做？ Decoder 的输入是什么？ Decoder 为什么要 Masked，如何实现的？ Transformer 的损失函数是什么？（softmax多分类的对数损失函数）
  
3. > 算法题

  第一题又是反转链表，反转链表这么火吗？第二题Leetcode原题，[分割回文串](https://leetcode-cn.com/problems/palindrome-partitioning/)，但我居然没写出来！和面试官交流了一下，其实思路是对的，但细节没处理好。
  
## 总结

作业帮的这位面试官人也很好，非常耐心，帮我一起改bug……似乎今年算法岗的“灰飞烟灭”没有网上说的那么严重，至少今天的两轮面试不算什么地狱难度。


# 2020.7.30 新浪算法工程师一面

面了一个多小时，面试官提的问题很有水平，也很理解应届生的水平的情况。面试官非常健谈，面试的过程几乎得有30%的时间是他在说，而我回答问题就像在抛砖引玉。面试官人好到我想和他做朋友！

1. > 自我介绍，介绍项目

  （项目还没介绍完就开始问我问题了）
  
2. > 让我介绍随机森林、GBDT、Lightgbm，想到啥说啥

  老问题了，叽里呱啦说了一大堆，从 Bagging, Boosting 角度说，从 Lightgbm、 XGboost的加速训练、正则化优化角度说。
  
3. > 面试官让我自己挑几个我比较熟的方向

  我说我对传统机器学习方法、深度的方法都还算熟，对NLP也有一定了解
  
4. > 开始问 CNN，问了一个卷积核、stride、padding和feature map大小关系的问题

  画了个图，简单推了一下，没啥问题
  
5. > 问空洞卷积，有什么作用？
  
  我回答出了扩大感受野的范围，但其实我不太记得空洞卷积和转置卷积的差别了，贴两篇文章在这儿
  
  [空洞卷积](https://zhuanlan.zhihu.com/p/50369448)
  
  [转置卷积](https://zhuanlan.zhihu.com/p/48501100)
  
5. > 问 RNN 怎么做 BP
  
  这个问题问得绝了，我只记得叫 BPTT，但没细看过……面试官回答说没关系，这个问题有点儿难
  
6. > 问文本怎么套 CNN ？
  
  像滑窗一样（类似 n-gram）
  
  > 长度是文本序列，那宽度是什么？
  
  不记得了，但灵机一动，说是 Embedding 后的维度（后来查了一下，好像TextCNN真是这么做的）
  
7. > 让我聊对 Transformer 的理解
  
  我从 RNN，LSTM 开始聊，聊到长距离依赖问题，然后说到 attention 和 self-attention 的区别，transformer 可以提特征 Balabala
  
  此时面试官和我聊为什么很多 Pre-trained 模型在 fine-tuning 过程中效果不好。我说可能预训练的数据和实际使用的数据差别比较大，比如 GPT2 在数据上花了很大功夫。面试官说了他的看法，有时候深度纯粹是玄学，可能 Encoder 或者 Decoder 少一层多一层效果就差很大，他建议应届生可以多实践一下。不过他也鼓励我说，没关系，应届生很多都不懂很正常，只要基础好，随便学学都能学会。当时非常感动，居然有这么善良的面试官！
  
8. > 考了几道编程题
  
  牛顿法求平方根（完全忘了牛顿法是什么了，和他交流了一下，还是不会……于是他转而让我写二分）
  （复盘了一下， $x_{n+1} = x_{n}-\frac{f(x_n)}{f'(x_n)}$）
  
  有序数组手写二分（这个还算简单），在有翻转的数组下写二分法（第二个花了点儿时间，但还是写出来了）
  

反问：

1. > 组里在做什么

  他在新浪新闻客户端，主要做文本分类，命名实体识别。对每条新闻，判断是否涉黄涉政，是否属于某个新闻板块下某个小板块。他的经验是，很多深度的东西总归还是不如规则来的简单方便。

2. > 问他为什么从百度跳到新浪
  
  他回答说跳出来歇歇脚，工作强度低一些，不过建议应届生还是要去大厂多学点儿东西。他举例说他们部门不比新浪微博部门，资源比较少，不是个很能锻炼人的地方，甚至不太建议我来新浪（听到这儿我简直要给他跪下，头一次听见面试官劝退求职者的，巨感动，巨良心！）
  
## 总结

面试以来的面试官一个比一个好，不过这是我面试以来遇见的最棒的面试官，没有之一！特地通过微信和他认真地道谢，面试的过程像是一位长者指引后辈前路一般，能够经历一次这种面试也是人生的幸运。


# 2020.7.31 大华算法工程师一面（音频方向）

相比于昨天新浪的神级面试官，今天的大华面试官就平凡很多。让我比较吃惊的是，全程几乎没问技术性问题……

先是自我介绍，之后问我用什么语言（R 和 python），问会不会 C 和 C++（学过 C ，忘了），套路和一面如出一辙。之后问实习项目，我都快把我的实习情况背下来了…但面试官老是喜欢打断我，让我有点儿不太爽。这位面试官是头一个问我论文的，我就简单和他说了一下什么是高斯过程回归，怎么定义协方差矩阵，怎么训练，怎么预测（有点儿开组会讲论文的感觉了）。

之后还问了几个很老套的问题，比如有什么兴趣爱好，压力比较大的时候会做什么，如果让你一个人负责一个比较紧急的项目你会怎么做（先多方面学习，多看论文博客源代码，然后硬着头皮做呗），还问我觉得我人生中比较成功的地方有哪些，比较失败的地方有哪些（头一次在技术面遇到这种问题，我以为得等到 HR 面才会被问）。最后问了我的职业规划（还是老套路）。

然后反问，我先问组里做什么（语音识别），是什么让面试官留在大华（开始画大饼，说可以接触到不同的项目，对于项目理解更深，成长更快）。最后明显觉得面试官有点儿想留我，说你统计机器学习建模挺熟练的，做这个应该没什么问题（这根本就不是一回事儿嘛）

# 2020.8.3 网易互娱机器学习工程师二面

面试官迟到二十分钟让我有点儿不爽，但面试开始后好歹面试官先道了个歉。这轮面试压力有一丢丢大，问的技术性问题比较密集且全面，但不算难。

1. > 自我介绍，介绍项目

2. > 编程语言用什么？会不会 C++ 和 JAVA ?

3. > 讲一下GBDT、XGBoost.

4. > DNN 一般怎么用？

  这个问题给我问懵了，什么叫怎么用？想了得有十秒钟左右，大概说了一下神经网络的线性层，激活函数，最终的分类和回归任务怎么做，还说了一下前向传播和后向传播。
  
5. > RNN 怎么做？

  RNN 相比于 DNN 最大的不同在于 RNN 的输入是一个有先后顺序的序列。RNN 的隐藏层会保留上一个输入传递后保留下的结果，从而使得网络可以捕捉到序列的关系。最后用 BPTT 对参数进行更新。
  
6. > 说一下怎么判断过拟合和欠拟合. 

  过拟合：训练误差明显小于测试误差；欠拟合：训练误差和测试误差都很大。
  
7. > Python 基本的数据结构有哪些？

  列表、元组、集合、字典
  
8. > 介绍一下 Python 的生成器和迭代器

  大体上回答出来了，但回答得不准确，贴一段 [菜鸟教程](https://www.runoob.com/python3/python3-iterator-generator.html) 的回答吧。
  
  迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。
  可用 `iter()` 创建迭代器, 可用 `next()` 返回迭代器的下一个元素。
  
  使用了 `yield` 的函数被称为生成器。生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。
  
9. > 了解 python 的垃圾回收机制吗

  不会，再引用一篇文章 [Python垃圾回收机制！非常实用 - 知乎](https://zhuanlan.zhihu.com/p/83251959).
  
  Python中，主要通过引用计数（Reference Counting）进行垃圾回收。在Python中，每一个对象的核心就是一个结构体PyObject，它的内部有一个引用计数器（ob_refcnt）。程序在运行的过程中会实时的更新ob_refcnt的值，来反映引用当前对象的名称数量。当某对象的引用计数值为0,那么它的内存就会被立即释放掉。
  

 以下情况是导致引用计数加一的情况：
- 对象被创建，例如a=2
- 对象被引用，b=a
- 对象被作为参数，传入到一个函数中
- 对象作为一个元素，存储在容器中

下面的情况则会导致引用计数减一:

- 对象别名被显示销毁 del
- 对象别名被赋予新的对象
- 一个对象离开他的作用域
- 对象所在的容器被销毁或者是从容器中删除对象

10. > linux 中 * 代表什么？

  零个或多个任意字符
  
11. > 讲一下线程和进程的关系

  得亏前段时间学到了，不过表达得还是不够清楚。
  
  以下内容均摘自知乎：
  
  [线程和进程的区别是什么？ - biaodianfu的回答 - 知乎](https://www.zhihu.com/question/25532384/answer/411179772)
  
  [每个程序员都会遇到的面试问题：谈谈进程和线程的区别 - Victor的文章 - 知乎](https://zhuanlan.zhihu.com/p/46410285)
  
  进程是资源分配的最小单位，进程是程序的一次执行过程，是一个动态概念，是程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间。
  
  线程是CPU调度的最小单位，它可与同属一个进程的其他的线程共享进程所拥有的全部资源。
  
  线程在进程下行进；一个进程可以包含多个线程；同一进程下不同线程间数据很易共享；任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。
  
  计算机操作系统里面有两个重要概念：并发和隔离。 并发是为了尽量让硬件利用率高，线程是为了在系统层面做到并发。线程上下文切换效率比进程上下文切换会高很多，这样可以提高并发效率。隔离也是并发之后要解决的重要问题，计算机的资源一般是共享的，隔离要能保障崩溃了这些资源能够被回收，不影响其他代码的使用。所以说一个操作系统只有线程没有进程也是可以的，只是这样的系统会经常崩溃而已，操作系统刚开始发展的时候和这种情形很像。
  
  > 进程之间如何通信？
  
  太难了，搜了以后我甚至没有要学的想法……不会也罢

12. > 说一下堆和栈这两种数据结构的差别

  栈是一种先进后出的数据结构，数据只能从栈顶压入和弹出。
  
  堆是一棵完全二叉树，树节点的值不小于子树节点的值。插入时往最后的位置插入，然后令其上浮；弹出时令根节点和尾节点交换，删除尾节点，并让根节点下沉。
  
13. > 讲 Attention，Transformer， Bert

  （现在面试都喜欢让求职者自己开始从头讲吗，我其实有点儿不太适应这种面试方式。从头讲我经常不知道从何说起）好在问的不深，都在我的掌握范围之内。
  
14. > 讲讲你了解的推荐算法

  讲了以下基于用户和基于物品的协同过滤。
  
  > 介绍一下FM，DeepFM
  
  回答得不是很好，再贴链接[因子分解机（FM）简介及实践 - 安超杰的文章 - 知乎](https://zhuanlan.zhihu.com/p/144346116)

15. > 最后出了一个巨简单的算法题，未知长度的链表，怎么找中点？

  快慢指针

  

反问

1. > 组里做什么？

  CC 直播，有推荐也有NLP和风控。
  
2. > 组里技术氛围如何？

  经常会有技术分享会
  
3. > 为什么一直留在网易？

  一直打马虎眼（我觉得他不太想回答，就没追问了）
  
## 总结

这轮面试问的问题够全的，把传统机器学习、NLP、推荐、深度学习、操作系统、Linux、Python、算法都问了一圈。好在三面邀请发的真够快的，半个小时后就收到了三面邀请。这轮面试下来我觉得需要再恶补一下 Bert 还有推荐系统了。

# 2020.8.4 网易互娱机器学习工程师 HR 面

昨天得知今天要进行三面，吓得我赶紧准备了不少新的技术性问题。结果今天上线时发现对方是 HR，而我还完全没对 HR 面做任何准备……

HR 面的氛围还是蛮轻松的，表面上属于聊天的状态，暗地里隐隐觉得有点儿杀机。

今天甚至没让我做自我介绍，开头就问我在实习期间大概做了什么事情。接着问了我是暑期实习还是日常实习，为什么没参加春招的暑期实习？（我说了疫情和论文两个方面的原因）

HR 问我打算参加讯飞的转正吗（我说应该会去试一下）

剩下的问题就比较随意了，HR 介绍了一下网易互娱的情况，我简单写一下权当备忘了。

网易互娱总部在广州，离最繁华的地段大概有十到二十分钟的车程；食堂包四餐（早中晚加夜宵，这是瞅准了要加班啊），有健身房和理疗室（都上理疗了，加班是多要命）；
公司文化比较务实，组内会经常调研最新的方法，但也还是会以业务为导向；技术氛围浓厚，会经常开展技术沙龙。
试用期六个月，六个月后述职定职级；每年有两次晋升的机会。
主要工作内容是 CC 直播的推荐以及 NLP 方向的工作（例如智能弹幕，对主播打标签之类的）。
不过薪资没说，HR 说目前还没定下来。

最后问了我的家庭情况以及是否有女朋友（估计是在猜我有多大可能会去广州）。我回答她说家里对我的就业没有任何要求，父母在央企工作，家庭情况稳定，没有女朋友（您就放心给我发 offer 好了）。

不知 HR 面会不会挂人，不过今天谈下来自我感觉良好，气氛不错，HR 看着也面善。希望一周后能先拿一个 offer 再说！（好歹也是猪场网易啊，而且这个工作岗位我真的蛮喜欢的。）



# 2020.8.8 奇安信算法工程师一面

面试官人挺和善的，二十分钟就面完了，不知道是面试官对我不感兴趣还是一面要求比较低，总之心里挺没底的。

问了项目，问了 Bagging 和 Boosting 的区别，问了简历上人工规则的细节。

最后最后问了一道算法题，挺意外的，问我不用 if 、while 和 for ，实现从 1 累加到 n。我一下子就想起 leetcode 曾经做过的那道题，抄起键盘开始写递归。

``` python
def func(n):
    return n and n+func(n-1)
```

写完之后面试官让我用 or 实现，想了几分钟，得到面试官的提示，终于给写出来了

```python
def func(n):
    return not n-1 and n+func(n-1)
```

然后面试官问我不编程的话你怎么实现，我一拍脑门：等差数列求和公式！

然后笑着和面试官承认刷题刷多了，刷成思维定势了，这也确实是个挺大的问题。

## 反问

1. >  问了奇安信的算法工程师具体的工作内容

   恶意流量和恶意软件的分类。用的技术偏向传统机器学习，也有用深度学习和 NLP 在做。





# 2020.8.10 小鹏汽车一面

有些太常规的问题实在不想记录了，记得多了也没什么意义，以后只记一些比较有难度或比较有思考价值的问题。

1. > 为什么 LSTM 能解决梯度消失问题

   我一时语塞，没答上来。后来才发现 LSTM 主要解决的就是梯度消失问题，而不是长距离依赖问题。

   后来查了不少博客，弄清楚了这个问题。

    [LSTM如何解决梯度消失或爆炸的？](https://www.cnblogs.com/bonelee/p/10475453.html)

   在 RNN 中， BPTT 会导致隐状态之间的激活函数求导（tanh 导数小于 1）导致梯度消失或梯度爆炸。

   而在 LSTM 中，因为采用了 gate 机制，所以反向传播时会被 gate 机制的 sigmoid 影响。
   Sigmoid 大多数情况都处于 0 或者 1 的状态，处于 1 时，梯度传播不受影响，而处于 0 时，说明上一时刻的信息对当前时刻没有影响， 我们也就没有必要传递梯度回去来更新参数了。

公司做智能语音的，包括车机的多轮人机对话，以及知识图谱。



# 2020.8.12 拼多多算法工程师一面

拼多多真拼啊，晚上九点面到十点多…问的问题很常规，一直在聊实习项目，最后出了两道算法题，一道是简单DP，一道有点儿难的DP，第二道在面试官的提醒下还是做出来了。



# 2020.8.15 贝壳找房数据挖掘/机器学习一面

聊项目聊了挺久，之后问的都还挺常规

问怎么防止过拟合；让我简述一下transformer；简述一下 LSTM，GRU 的改进；了解哪些优化器，Adam 的改进；



编程题：[LeetCode 三数之和](https://leetcode-cn.com/problems/3sum/)

以及问我`a :[2,3],b:[2,3]` `torch.cat([a,b],dim = 0)` 和 `torch.stack([a,b],dim = 0)`后 Tensor 的大小
第一次见有问 Pytorch 的，`[4,3]` 和 `[2,2,3]`



# 2020.8.15 贝壳找房数据挖掘/机器学习二面

面试官感觉像个 Boss，说话有种特殊的气场。主要在聊项目，最后给了两道编程题，一个是字符串的全排列，一个是二分法找数字。

第一题用回溯法轻松解决，第二题原本也很简单，但面试官要求有点儿高，一定要让我写 `left, right = 0, size` 的情形。

好在顺利通过了，几分钟后收到了三面邮件



# 2020.8.18 携程算法工程师一面

第一位在技术面遇见女面试官，我震惊了。虽然是个可爱的女孩子，但问的问题还蛮难的。

1. >  XGBoost 缺失值怎么处理

   训练时不考虑有缺失值的样本，把没有缺失值的样本分别分在左子树和右子树，选择更优的一边。按照论文中的说法，在预测时会固定把样本分到右子树。

2. > 比较SVM和LightGBM

   这种问题一下子竟然不知道怎么回答…大概把两种模型的特点说了一下。

3. > 说说 Transformer 

   几乎是今年最常考的问题之一

4. > 让我手写 pytorch  代码

   这个倒不难，写了一个分类的 DNN

5. 还有一个编程题，判断一个整数是否是对称的。

   逐元素添加到列表，然后判断列表是否是对称的就 OK 了。

反问：

统招统分，可以按照自己的意愿选择部门（但携程今年有点儿困难，不知道招不招人）

面试官所在的部门是做酒店相关的推荐和 NLP 的（智能客服）。



# 2020.8.19 阿里巴巴算法工程师一面

阿里足足面了一小时四十分钟，问题不算难，但考察的面巨广。从数学到算法再到统计机器学习和深度学习都问了个遍。

1. > 自我介绍

   介绍完只简短地聊了一下实习，并没有深入

2. > 说一下矩阵的秩

   我说矩阵的秩大概形容了矩阵的信息量（其实不准确），如果方阵非满秩，那么肯定有线性相关的行或者列，可以理解成没有提供额外的信息。如果矩阵满秩则矩阵可逆，是一个很好的性质。

   > 再说一下矩阵的特征值和特征向量

	矩阵代表着一种投影，如果矩阵作用于特征向量上，那么会把特征向量以特征值的倍数进行拉伸放缩。

3. > 讲一下主成分分析

4. > 一个总数很大的数据流，希望以等概率的方式进行样本数为一的抽样，怎么做？

   我讲了一下蓄水池抽样，面试官要求我继续深入证明一下（用数学归纳法）。

   讲完之后面试官表示可以，然后要求我再想一个方法，然而想了两分钟也没有想出来。

   最后面试官和我说了他的方法：每拿到一个数就生成一个随机数，如果新随机数比现有的随机数更大，则替换数据。然后面试官要求我证明了一下，我解释说：可以理解成每条数据背后都隐含一个已经生成了的随机数，只是我们目前没有观察到。在经手前 k 个数据后，每个数据所代表的随机数是最大值的概率都是相等的，所以最后每条数据被保留的概率也是等可能的。（秒啊！）

5. > 考了一道编程题，LeetCode 的接雨水

   我讲了一下双向动规然后取最小值的解法。不过这道题并没有结束，面试官鼓励我尝试用其他算法来解答。我又想出了用单调栈的方法，面试官继续让我想一个只访问一次的方法。我当时有点儿被单向遍历给框住想象力了，怎么也想不出只访问一次的方法。接下来面试官提示我用双指针，恍然大悟，最后愉快地写了代码，面试官表示可以。

接下来问的都是机器学习算法相关的问题了。

6. > 树模型怎么分裂？

   我讲了一下 ID3，C4.5 还有 Cart 的分裂指标。

   > 怎么理解信息增益？

   信息增益就是当前的熵减去分裂后的条件熵，熵表示数据混乱的程度，如果分裂十分有效，则左子树和右子树中的 label 都是很统一的，这样会使得信息增益最大。

   > 讲一下交叉熵的公式和意义

   被问过许多遍了，当然不在话下。

   > 为什么分类问题的损失函数用交叉熵？

   分类问题最终往往会套一个 Sigmoid 或者 Softmax，求导时会使得梯度很小，参数更新很慢。

7. > 随机森林怎么提高泛化能力的？

   我说了一下样本和特征层面的抽样，面试官表示还有呢？我又说了基学习器之间是独立的，所以最后取平均可以起到减小模型方差的效果，面试官仍然不满意。我想了一下，说还可以通过调整基学习器的树的深度来减小基学习器的过拟合风险。其他的实在想不出来了…

   > 讲一下GBDT，Gradient 代表什么？

   每一次迭代，模型是沿着当前损失函数的负梯度方向学习一个新的学习器，在均方误差的意义下也就是当前模型和真实值的残差。

8. > 说一下牛顿法，为什么深度学习很少用牛顿法？

   牛顿法使用到了二阶的梯度信息，会有收敛于鞍点的风险，同时需要计算 Heissen 矩阵的逆，时间复杂度比较高。（面试官表示继续，我也实在不知道还有什么原因了）

   > 牛顿法一般应用于什么场景，有什么好处？

   损失函数是凸函数时可以使用牛顿法，好处在于收敛很快，迭代次数少。

9. >  神经网络怎么避免过拟合？

   我说了 Early stopping，Dropout，BN，减小网络层数，增加参数的正则项。（面试官还让我继续，我又不知道还有什么方法了）

10. > 说一下生成式模型

    不同于判别式模型，生成式模型是对特征和 label 的联合分布建模。

    > 生成式模型和判别式模型有什么特点？

    生成式模型对联合分布建模，所以在样本量较大时比较有效，而样本量较小时不足以精确地描述联合概率密度。而判别式模型只关注决策面，所以对于小样本也比较有效。

    > 生成式和判别式模型哪个使用极大似然估计？

    判别式模型