# 2020.7.20 浙江大华算法工程师一面（CV方向）

今天秋招面试开张，面试体验良好，只是一开始问的几个数学问题实在没想到，好在自我感觉回答得还行。

做完自我介绍后面试官像是早有预谋般地问“条件随机场了解吗”（大概说了一下属于判别方法，极大团，然后就说不出来了。我每次都抱着侥幸的心理不想看CRF，甚至莫名地排斥这个方法，今天墨菲定律终于显灵了。）

之后问的问题完全没有条理性，纯属想到哪儿问到哪儿。大概问题如下：

1. > Hessian 矩阵了解吗，介绍一下 Heissian 矩阵与鞍点的关系
  
  函数对自变量求二阶导；特征值有正有负
  
2. > 矩阵可逆的条件

  （线代为数不多还记得的知识点）列向量线性无关、矩阵满秩、特征值没有 0 ，行列式不为 0，这几个互相等价
  
3. > 介绍一下交叉熵，还有哪些衡量分布差异的量

  KL 散度
  
  > 介绍一下 KL 散度
  
  记不清了，只记得大概形式与交叉熵有关，面试完翻了翻书： $H(p,q) = D_{KL}(P||Q)+H(P)$
  
4. > 介绍一下权重衰减

（简单说了一下，但没让解释）先验分布和解空间形状两个方面

5. > 了解哪些网络结构

  前馈，CNN，RNN，残差网络，LSTM，GRU，Seq2seq
  
6. > 了解轻量级网络结构吗

  没听过……
  
7. > 说一下 Sigmoid 为什么用的不多，有什么缺点

  和 tanh 一样，会饱和，导致梯度消失
  
8. > C 和 C++ 会吗？

  不会

9. > 平时用什么语言

  python 和 R
  
  > 用过哪些神经网络框架？
  
  pytorch

10. > 出了一道巨简单的考察 Python list 复制操作的题

11. > 深拷贝和浅拷贝

  浅拷贝只做最顶层的数据类型判断，深拷贝做递归拷贝；遇到可变类型则创建新的内存空间，遇到不可变数据类型就是拷贝的引用。（之前平安科技也面过这个问题，没想到这么常考）
  
12. > Linux 会吗？列出文件目录是什么命令？按时间顺序列出文件？统计目录下文件个数？

  （后俩都不会，linux 基础太差了）
  
13. > 说一下残差网络，在哪儿学的？
    
   博客和书，然后说了一下有些CNN结构也常用，transformer也用到了
    
14. > pytorch 里哪些结构在训练和测试阶段有区别？

  dropout 和 bn，自我感觉解释得挺好的，把 train 和 eval 也说了
  
15. > CNN 怎么做 BN？

  就按 batch size 压缩呗

16. > BN 最后有个线性变换，为什么加线性变换？

  （如果后面接 Sigmoid，会损失非线性效果。由数据学习变换的参数，可以提高网络表达能力）
  
17. > 你觉得你有什么理由说服我你能胜任这个岗位？

对自己的学习能力一顿乱夸

18. > 介绍一下实习项目

  balabala 侃了一大堆，结果也没问相关的问题…
  
19. > 学过哪些和机器学习相关的课程？

  统计学习、算法导论、数据结构……
  
20. > 给定数据 X 和 label Y，用线性回归怎么求解权重参数？

  正规方程 $W = (X^\top X)^{-1}X^\top Y$，亏我还记得
  
21. > 还用过哪些基于梯度的优化算法？

Adagrad，RMSProp，Adam，Adam 用的比较多，效果最好

  > Adam 为什么效果好，有哪些改进？
  
  Adam 基于 RMSPorp，每个参数有一个自适应的learning rate，同时累计了一阶和二阶梯度的信息，还进行了无偏的修正。
  
  > 基于动量的方法有什么好处？
  
  基于动量的方法可以累积历史的梯度，加速训练，同时还能跃过一些局部极小点。
  
22. > 讲一下 boosting

  boosting 是一个集成学习方法，由基学习器累加而成。通过每一步学习一个基学习器，最终得到一个精度较高的学习器。
  
  > 和随机森林有什么不同？
  
  随机森林是基于 bagging 方法的模型，可以并行训练，而 boosting 只能串行训练。随机森林的每个学习器都是一棵低偏差高方差的分类或回归树，通过取平均来得到最终的结果。这样取平均可以降低模型的方差，避免过拟合。
  
23. > 有什么问题要问我？

   问了组里做的工作（目标检测，人脸识别，活体检测）；预研阶段用 python ，落地用 C 或者 C++；虽然用的大多都是 deep 的方法，但也希望我多了解传统计算机视觉的方法和思想。
   
##  总结

面试官问的问题难度尚可，面试时长一个小时，没问什么奇奇怪怪的问题。除了上述问题以外，还问过我看没看过某篇论文，虽然最近抽空看了不少论文和源码，但总做不到面面俱到。不会的问题我都老老实实交代，免得被追问起来更尴尬。不过今天面试过程中我和面试官之间几乎没有双向的交流，他问我答，略微有点儿压力。好在秋招第一面下来，自己发现了不少问题，可以早点儿查漏补缺以免后面挂在心仪的公司上。


# 2020.7.29 网易互娱机器学习工程师一面

面试官人很nice，问的问题很基础，大概半个小时结束。

1. > 自我介绍、介绍项目

2. > 缺失值怎么处理？

  可以采用树模型这类对缺失值不敏感的模型；采用均值或中位数填充；也可以采用规则，分层进行填充；（当时忘了说把缺失值单独作为一类，新提一个特征了）
  
3. > 介绍stacking集成学习

4. > 反转链表（不额外占用空间）

  用递归实现了（后来想了一下，其实可以额外开一个dummy 节点，重新创建一个链表，倒也不算额外占用空间）
  
5. > 一道 SQL 题（不会）

6. > 手写逻辑回归的损失函数和导数

7. > 一道开放题，给定一个黑盒神经网络（结构未知），输入是一个double数组，问如何找到神经网络的最小输出值

  想到了 SMO 算法和坐标轴下降法，大概说了一下固定其他输入的维数，只单独对某一个维度的输入进行线性搜索，找到其极小值点，然后迭代直至收敛。
  
## 总结

总体还是比较简单轻松的，甚至比找实习的一堆面试还要简单。忍不住再夸一遍面试官，人真的不错，很和蔼，面试起来也没什么压力，甚至能够享受面试中和面试官交流的过程。

# 2020.7.29 作业帮机器学习算法工程师一面

面试长达一小时，也还比较轻松

1. > 自我介绍，介绍项目

  面试官对于业务挖得有点儿深（好奇是不是打探机密的），问 lightgbm 和随机森林的区别，lightgbm 如何分裂，lightgbm 的优化之处在哪儿
  
2. > Transformer

  我给自己挖了个坑，还好又跳出来了。面试官问有没有了解深度学习方面的东西，我回答说前馈神经网络、CNN、RNN，甚至深度的NLP模型都有了解。然后面试官开始挖 Transformer，问Transformer 解决了什么问题，Transformer 的 self-attention 怎么做？ Decoder 的输入是什么？ Decoder 为什么要 Masked，如何实现的？ Transformer 的损失函数是什么？（softmax多分类的对数损失函数）
  
3. > 算法题

  第一题又是反转链表，反转链表这么火吗？第二题Leetcode原题，[分割回文串](https://leetcode-cn.com/problems/palindrome-partitioning/)，但我居然没写出来！和面试官交流了一下，其实思路是对的，但细节没处理好。
  
## 总结

作业帮的这位面试官人也很好，非常耐心，帮我一起改bug……似乎今年算法岗的“灰飞烟灭”没有网上说的那么严重，至少今天的两轮面试不算什么地狱难度。


# 2020.7.30 新浪算法工程师一面

面了一个多小时，面试官提的问题很有水平，也很理解应届生的水平的情况。面试官非常健谈，面试的过程几乎得有30%的时间是他在说，而我回答问题就像在抛砖引玉。面试官人好到我想和他做朋友！

1. > 自我介绍，介绍项目

  （项目还没介绍完就开始问我问题了）
  
2. > 让我介绍随机森林、GBDT、Lightgbm，想到啥说啥

  老问题了，叽里呱啦说了一大堆，从 Bagging, Boosting 角度说，从 Lightgbm、 XGboost的加速训练、正则化优化角度说。
  
3. > 面试官让我自己挑几个我比较熟的方向

  我说我对传统机器学习方法、深度的方法都还算熟，对NLP也有一定了解
  
4. > 开始问 CNN，问了一个卷积核、stride、padding和feature map大小关系的问题

  画了个图，简单推了一下，没啥问题
  
5. > 问空洞卷积，有什么作用？
  
  我回答出了扩大感受野的范围，但其实我不太记得空洞卷积和转置卷积的差别了，贴两篇文章在这儿
  
  [空洞卷积](https://zhuanlan.zhihu.com/p/50369448)
  
  [转置卷积](https://zhuanlan.zhihu.com/p/48501100)
  
5. > 问 RNN 怎么做 BP
  
  这个问题问得绝了，我只记得叫 BPTT，但没细看过……面试官回答说没关系，这个问题有点儿难
  
6. > 问文本怎么套 CNN ？
  
  像滑窗一样（类似 n-gram）
  
  > 长度是文本序列，那宽度是什么？
  
  不记得了，但灵机一动，说是 Embedding 后的维度（后来查了一下，好像TextCNN真是这么做的）
  
7. > 让我聊对 Transformer 的理解
  
  我从 RNN，LSTM 开始聊，聊到长距离依赖问题，然后说到 attention 和 self-attention 的区别，transformer 可以提特征 Balabala
  
  此时面试官和我聊为什么很多 Pre-trained 模型在 fine-tuning 过程中效果不好。我说可能预训练的数据和实际使用的数据差别比较大，比如 GPT2 在数据上花了很大功夫。面试官说了他的看法，有时候深度纯粹是玄学，可能 Encoder 或者 Decoder 少一层多一层效果就差很大，他建议应届生可以多实践一下。不过他也鼓励我说，没关系，应届生很多都不懂很正常，只要基础好，随便学学都能学会。当时非常感动，居然有这么善良的面试官！
  
8. > 考了几道编程题
  
  牛顿法求平方根（完全忘了牛顿法是什么了，和他交流了一下，还是不会……于是他转而让我写二分）
  （复盘了一下， $x_{n+1} = x_{n}-\frac{f(x_n)}{f'(x_n)}$）
  
  有序数组手写二分（这个还算简单），在有翻转的数组下写二分法（第二个花了点儿时间，但还是写出来了）
  
反问：

1. > 组里在做什么

  他在新浪新闻客户端，主要做文本分类，命名实体识别。对每条新闻，判断是否涉黄涉政，是否属于某个新闻板块下某个小板块。他的经验是，很多深度的东西总归还是不如规则来的简单方便。
 
2. > 问他为什么从百度跳到新浪
  
  他回答说跳出来歇歇脚，工作强度低一些，不过建议应届生还是要去大厂多学点儿东西。他举例说他们部门不比新浪微博部门，资源比较少，不是个很能锻炼人的地方，甚至不太建议我来新浪（听到这儿我简直要给他跪下，头一次听见面试官劝退求职者的，巨感动，巨良心！）
  
## 总结

面试以来的面试官一个比一个好，不过这是我面试以来遇见的最棒的面试官，没有之一！特地通过微信和他认真地道谢，面试的过程像是一位长者指引后辈前路一般，能够经历一次这种面试也是人生的幸运。
  
  
# 2020.7.31 大华算法工程师一面（音频方向）

相比于昨天新浪的神级面试官，今天的大华面试官就平凡很多。让我比较吃惊的是，全程几乎没问技术性问题……

先是自我介绍，之后问我用什么语言（R 和 python），问会不会 C 和 C++（学过 C ，忘了），套路和一面如出一辙。之后问实习项目，我都快把我的实习情况背下来了…但面试官老是喜欢打断我，让我有点儿不太爽。这位面试官是头一个问我论文的，我就简单和他说了一下什么是高斯过程回归，怎么定义协方差矩阵，怎么训练，怎么预测（有点儿开组会讲论文的感觉了）。

之后还问了几个很老套的问题，比如有什么兴趣爱好，压力比较大的时候会做什么，如果让你一个人负责一个比较紧急的项目你会怎么做（先多方面学习，多看论文博客源代码，然后硬着头皮做呗），还问我觉得我人生中比较成功的地方有哪些，比较失败的地方有哪些（头一次在技术面遇到这种问题，我以为得等到 HR 面才会被问）。最后问了我的职业规划（还是老套路）。

然后反问，我先问组里做什么（语音识别），是什么让面试官留在大华（开始画大饼，说可以接触到不同的项目，对于项目理解更深，成长更快）。最后明显觉得面试官有点儿想留我，说你统计机器学习建模挺熟练的，做这个应该没什么问题（这根本就不是一回事儿嘛）

# 2020.8.3 网易互娱机器学习工程师二面

面试官迟到二十分钟让我有点儿不爽，但面试开始后好歹面试官先道了个歉。这轮面试压力有一丢丢大，问的技术性问题比较密集且全面，但不算难。

1. > 自我介绍，介绍项目

2. > 编程语言用什么？会不会 C++ 和 JAVA ?

3. > 讲一下GBDT、XGBoost.

4. > DNN 一般怎么用？

  这个问题给我问懵了，什么叫怎么用？想了得有十秒钟左右，大概说了一下神经网络的线性层，激活函数，最终的分类和回归任务怎么做，还说了一下前向传播和后向传播。
  
5. > RNN 怎么做？

  RNN 相比于 DNN 最大的不同在于 RNN 的输入是一个有先后顺序的序列。RNN 的隐藏层会保留上一个输入传递后保留下的结果，从而使得网络可以捕捉到序列的关系。最后用 BPTT 对参数进行更新。
  
6. > 说一下怎么判断过拟合和欠拟合. 

  过拟合：训练误差明显小于测试误差；欠拟合：训练误差和测试误差都很大。
  
7. > Python 基本的数据结构有哪些？

  列表、元组、集合、字典
  
8. > 介绍一下 Python 的生成器和迭代器

  大体上回答出来了，但回答得不准确，贴一段 [菜鸟教程](https://www.runoob.com/python3/python3-iterator-generator.html) 的回答吧。
  
  迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。
  可用 `iter()` 创建迭代器, 可用 `next()` 返回迭代器的下一个元素。
  
  使用了 `yield` 的函数被称为生成器。生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。
  
9. > 了解 python 的垃圾回收机制吗

  不会，再引用一篇文章 [Python垃圾回收机制！非常实用 - 知乎](https://zhuanlan.zhihu.com/p/83251959).
  
  Python中，主要通过引用计数（Reference Counting）进行垃圾回收。在Python中，每一个对象的核心就是一个结构体PyObject，它的内部有一个引用计数器（ob_refcnt）。程序在运行的过程中会实时的更新ob_refcnt的值，来反映引用当前对象的名称数量。当某对象的引用计数值为0,那么它的内存就会被立即释放掉。
  
 以下情况是导致引用计数加一的情况：
- 对象被创建，例如a=2
- 对象被引用，b=a
- 对象被作为参数，传入到一个函数中
- 对象作为一个元素，存储在容器中

下面的情况则会导致引用计数减一:

- 对象别名被显示销毁 del
- 对象别名被赋予新的对象
- 一个对象离开他的作用域
- 对象所在的容器被销毁或者是从容器中删除对象

10. > linux 中 * 代表什么？

  零个或多个任意字符
  
11. > 讲一下线程和进程的关系

  得亏前段时间学到了，不过表达得还是不够清楚。
  
  以下内容均摘自知乎：
  
  [线程和进程的区别是什么？ - biaodianfu的回答 - 知乎](https://www.zhihu.com/question/25532384/answer/411179772)
  
  [每个程序员都会遇到的面试问题：谈谈进程和线程的区别 - Victor的文章 - 知乎](https://zhuanlan.zhihu.com/p/46410285)
  
  进程是资源分配的最小单位，进程是程序的一次执行过程，是一个动态概念，是程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间。
  
  线程是CPU调度的最小单位，它可与同属一个进程的其他的线程共享进程所拥有的全部资源。
  
  线程在进程下行进；一个进程可以包含多个线程；同一进程下不同线程间数据很易共享；任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。
  
  计算机操作系统里面有两个重要概念：并发和隔离。 并发是为了尽量让硬件利用率高，线程是为了在系统层面做到并发。线程上下文切换效率比进程上下文切换会高很多，这样可以提高并发效率。隔离也是并发之后要解决的重要问题，计算机的资源一般是共享的，隔离要能保障崩溃了这些资源能够被回收，不影响其他代码的使用。所以说一个操作系统只有线程没有进程也是可以的，只是这样的系统会经常崩溃而已，操作系统刚开始发展的时候和这种情形很像。
  
  > 进程之间如何通信？
  
  太难了，搜了以后我甚至没有要学的想法……不会也罢

12. > 说一下堆和栈这两种数据结构的差别

  栈是一种先进后出的数据结构，数据只能从栈顶压入和弹出。
  
  堆是一棵完全二叉树，树节点的值不小于子树节点的值。插入时往最后的位置插入，然后令其上浮；弹出时令根节点和尾节点交换，删除尾节点，并让根节点下沉。
  
13. > 讲 Attention，Transformer， Bert

  （现在面试都喜欢让求职者自己开始从头讲吗，我其实有点儿不太适应这种面试方式。从头讲我经常不知道从何说起）好在问的不深，都在我的掌握范围之内。
  
14. > 讲讲你了解的推荐算法

  讲了以下基于用户和基于物品的协同过滤。

15. > 最后出了一个巨简单的算法题，未知长度的链表，怎么找中点？

  快慢指针
  
反问

1. > 组里做什么？

  CC 直播，有推荐也有NLP和风控。
  
2. > 组里技术氛围如何？

  经常会有技术分享会
  
3. > 为什么一直留在网易？

  一直打马虎眼（我觉得他不太想回答，就没追问了）
  
## 总结

这轮面试问的问题够全的，把传统机器学习、NLP、推荐、深度学习、操作系统、Linux、Python、算法都问了一圈。好在三面邀请发的真够快的，半个小时后就收到了三面邀请。这轮面试下来我觉得需要再恶补一下 Bert 还有推荐系统了。
